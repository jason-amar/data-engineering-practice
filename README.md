# Data Engineering Bootcamp

My learning journey to become a data engineer, focusing on Azure/Databricks stack.

## 📚 Curriculum

16-week intensive program covering:
- Python & SQL foundations
- PySpark & Databricks
- Azure data services
- Data modeling & warehousing
- Orchestration & CI/CD
- Production best practices

## 🗂️ Project Structure
```
data-engineering-bootcamp/
├── week1-setup/
│   ├── venv/
│   ├── logger_demo.py
│   ├── requirements.txt
│   ├── app.log
│   └── .gitignore
├── .gitignore
└── README.md
```

## 🚀 Setup Instructions

1. Clone this repository
2. Create virtual environment: `python -m venv venv`
3. Activate venv:
   - Windows: `venv\Scripts\activate`
   - Mac/Linux: `source venv/bin/activate`
4. Install dependencies: `pip install -r requirements.txt`

## 📅 Progress Tracker

- [x] Week 1, Day 1: Development Environment Setup
- [x] Week 1, Day 2: Working with File Formats
- [ ] Week 1, Day 3: API Data Extraction
- [ ] Week 1, Day 4: Data Validation
- [ ] Week 1, Day 5: Mini ETL Pipeline

## 💻 Technologies

- Python 3.11+
- Pandas, Requests, Pytest
- Git & GitHub
- VS Code

## 📝 Learning Notes

### Week 1: Python & SQL Foundations

**Day 1 - Development Environment Setup**
- Set up professional dev environment
- Learned Python logging best practices
- Configured Git with proper .gitignore
- Created reproducible environment with requirements.txt

**Day 2 - Working with File Formats**
- Compared CSV, JSON, and Parquet performance
- Learned why Parquet dominates big data (columnar storage, compression)
- Built reusable file format converter
- Benchmarked read/write speeds and file sizes

---

*Last Updated: October 20, 2025*