# Data Engineering Bootcamp

My learning journey to become a data engineer, focusing on Azure/Databricks stack.

## ğŸ“š Curriculum

16-week intensive program covering:
- Python & SQL foundations
- PySpark & Databricks
- Azure data services
- Data modeling & warehousing
- Orchestration & CI/CD
- Production best practices

## ğŸ—‚ï¸ Project Structure
```
data-engineering-bootcamp/
â”œâ”€â”€ week1-setup/
â”‚   â”œâ”€â”€ venv/
â”‚   â”œâ”€â”€ logger_demo.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.log
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Setup Instructions

1. Clone this repository
2. Create virtual environment: `python -m venv venv`
3. Activate venv:
   - Windows: `venv\Scripts\activate`
   - Mac/Linux: `source venv/bin/activate`
4. Install dependencies: `pip install -r requirements.txt`

## ğŸ“… Progress Tracker

- [x] Week 1, Day 1: Development Environment Setup
- [x] Week 1, Day 2: Working with File Formats
- [ ] Week 1, Day 3: API Data Extraction
- [ ] Week 1, Day 4: Data Validation
- [ ] Week 1, Day 5: Mini ETL Pipeline

## ğŸ’» Technologies

- Python 3.11+
- Pandas, Requests, Pytest
- Git & GitHub
- VS Code

## ğŸ“ Learning Notes

### Week 1: Python & SQL Foundations

**Day 1 - Development Environment Setup**
- Set up professional dev environment
- Learned Python logging best practices
- Configured Git with proper .gitignore
- Created reproducible environment with requirements.txt

**Day 2 - Working with File Formats**
- Compared CSV, JSON, and Parquet performance
- Learned why Parquet dominates big data (columnar storage, compression)
- Built reusable file format converter
- Benchmarked read/write speeds and file sizes

---

*Last Updated: October 20, 2025*